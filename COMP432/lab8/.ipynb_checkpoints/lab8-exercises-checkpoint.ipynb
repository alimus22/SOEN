{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Cross Validation and Hyperparameter Search\n",
    "\n",
    "In this lab you'll use *cross validation* to do *hyperparameter search*, a particular form of *model selection*.\n",
    "* **Cross validation** is a procedure for *estimating* the test-time performance of a given model. Cross validation does not need access to any test data. Instead, it works by holding out some of the training data and using if it were testing data. In this lab we will focus on *K*-fold cross validation, where the training data is divided into *K* equally-sized chunks and each chunk gets a turn \"pretending\" to be a test set. This will be explained.\n",
    "* **Model selection** procedures try to select a \"best\" model from among many alternatives. The best model is usually the one that we can *expect* to have the best test-time performance. Since cross validation lets us *estimate* the test-time performance, many model selection procedures rely on cross validation as a subroutine.\n",
    "* **Hyperparameter search** is a particular *model selection* procedure that focuses on selecting one model from a family of related models. Hyperparameter search can therefore be seen as a \"model tuning\" procedure. For example, we can define all *DecisionTreeClassifiers* with *max_depth* $\\in\\{1,\\ldots,1000\\}$ as being a \"family\" with 1000 possible members to choose from. Hyperparameter search would find a particular setting for *max_depth* that is estimated to have the best possible test-time performance (*not* necessarily the best training performance).  A good hyperparameter search procedure will find good hyperparameters (hyperparameters estimated to have good test-time performance) using very few \"attempts\", since each attempt requires performing cross validation and this can be computationally demanding.\n",
    "\n",
    "The goal is to understand what cross validation is, how it is used as a subroutine within a hyperparameter search procedure. You will be asked to try two different hyperparameter search procedures: grid search and random search.\n",
    "\n",
    "**Run the code cell below** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.svm               # For SVC\n",
    "import sklearn.tree              # For DecisionTreeClassifier\n",
    "import sklearn.metrics           # For accuracy_score\n",
    "import sklearn.model_selection   # For cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "import scipy\n",
    "import scipy.stats               # For reciprocal distribution\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 1. Understanding Cross Validation\n",
    "\n",
    "Exercise 1.1&ndash;1.3 ask you to load and preprocess the training data (**exercise1_train.csv**), then split the data to training set and held-out test set (**[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)**), then finally compare how *K*-fold cross validation can estimate the held-out test performance of a model.\n",
    "\n",
    "**Run the code cell below** to define some functions for plotting data and model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise1_extent = (-3, 4, -3, 4)\n",
    "\n",
    "def plot_decision_function(model):\n",
    "    \"\"\"\n",
    "    Plots the decision function of a model as a red-blue heatmap.\n",
    "    The region evaluated, along with x and y axis limits, are determined by 'extent'.\n",
    "    \"\"\"\n",
    "    x1min, x1max ,x2min, x2max = exercise1_extent\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 200), np.linspace(x2min, x2max, 200))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    y = model.decision_function(X).reshape(x1.shape)\n",
    "    plt.imshow(-y, extent=exercise1_extent, origin='lower', vmin=-1, vmax=1, cmap='bwr', alpha=0.5, interpolation='nearest')\n",
    "    if y.min() < 0 and y.max() > 0:\n",
    "        plt.contour(x1, x2, y, levels=[0], colors=['k'])  # Decision boundary\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    \n",
    "def plot_data(X, y):\n",
    "    \"\"\"Plots the data from Exercise 1\"\"\"\n",
    "    plt.scatter(*X[y==0].T, marker=\"x\", c=\"r\")\n",
    "    plt.scatter(*X[y==1].T, marker=\"x\", c=\"b\")\n",
    "    plt.xlim(exercise1_extent[:2])\n",
    "    plt.ylim(exercise1_extent[2:])\n",
    "    plt.gca().set_aspect('equal')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.1 &mdash;  Load, re-scale, and plot full data set\n",
    "\n",
    "Start by loading the data from exercise1_train.csv, then seperate the data and also pre-process the data to the right scale, and plot the data using **plot_data()** function. \n",
    "\n",
    "Your plot should look like the figure below.\n",
    "\n",
    "![image](img/data.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 6-8 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.2 &mdash;  Split data to training and testing\n",
    "In this exercise, you should use  **[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)** to split the data from Exercise 1.1 to two groups: a training set, and a held-out test set. Use *random_state*=0. For this exercise, you should explicitly ask for a 70/30 split (70% training data, 30% testing data). Plot the two data sets side-by-side, using the *subplot* function of Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 6-10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "### Exercise 1.3 &mdash; Use *K*-fold cross-validation to *estimate* held-out performance\n",
    "\n",
    "As a rule, data marked as a \"test set\" should ALMOST NEVER be used for training, or even for model selection. All modeling choices (parameters, best model) must be made based on training data, ONLY. Otherwise you will very likely fool yourself, or others, into thinking your system will perform well on held-out data when it will not. \n",
    "\n",
    "\"Peeking\" at the test data, directly or indirectly, or even measuring the performance on test data too often, is even considered cheating. In fact, at least <a href=\"https://www.cio.com/article/2935233/baidu-fires-researcher-involved-in-ai-contest-flap.html\">one well-known machine learning scientist was <b>fired from his job</b></a> for trying to tune hyperparameters directly to the test data.\n",
    "\n",
    "\n",
    "***K*-fold cross validation** is a specific procedure for estimating held-out performance using only the training set. It creates *K* different (training, validation) splits and then averaging the validation performance measured on each one. (Beware that scikit-learn's [desciption of cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold) sometimes refers to the *K* individual validation sets as \"test sets\" so this can be confusing since they are not really validation sets.) The *K*-fold cross validation procedure is depicted below. When there are *K* splits the result is *K* different performance estimates, one for each of the held-out folds.\n",
    "<img src=\"img/k-fold.png\" width=\"550\">\n",
    "\n",
    "(Image source: https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "Note that the \"test data\" depicted above is not needed for the cross validation procedure itself, and is only used as an (optional) final performance evaluation, after the model selection procedure.\n",
    "\n",
    "**Write a few lines of code** to\n",
    "1. train a *DecisionTreeClassifier* with *max_depth*=10\n",
    "2. print the accuracy on the training set and test set from Exercise 1.2\n",
    "2. print the *K*-fold cross validation accuracy for each $K \\in \\{2,\\ldots,9\\}$.\n",
    "\n",
    "Use the **[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)** function or, equivalently, the **[score](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.score)** method of your *DecisionTreeClassifier* to compute the training and testing accuracies.\n",
    "\n",
    "Use the **[sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)** function to do the cross validation. It will return an array of *K* values, so you need to average them to get an overall estimate.\n",
    "\n",
    "Your code should print output that looks like this:\n",
    "```\n",
    "training accuracy:           100.0%\n",
    "held-out accuracy (testing):  ??.?%\n",
    "held-out accuracy (2-fold):   ??.?%\n",
    "held-out accuracy (3-fold):   ??.?%\n",
    "held-out accuracy (4-fold):   ??.?%\n",
    "held-out accuracy (5-fold):   ??.?%\n",
    "held-out accuracy (6-fold):   ??.?%\n",
    "held-out accuracy (7-fold):   ??.?%\n",
    "held-out accuracy (8-fold):   ??.?%\n",
    "held-out accuracy (9-fold):   ??.?%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 6-10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask yourself: are the *K*-fold cross validation accuracies a reasonable estimate of the testing accuracy? Are they at least a better estimate than *training* accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 2. Hyperparameter search\n",
    "\n",
    "Hyperparameter search is a particular kind of model selection, where we tune the parameters that are normally considered \"fixed\" (constant) during training itself.\n",
    "\n",
    "Exercise 2.1 asks you to plot the SVM model with different hyperparameters, this would help you to visualize the grid of hyperparameters.\n",
    "\n",
    "Exercise 2.2 asks you to perform grid search using **[sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)**\n",
    "\n",
    "Exercise 2.3 asks you to perform random hyperparameter search\n",
    "\n",
    "Exercise 2.4 asks you to evaluate the model selected from exercise 1.4 and 1.5 on the held-out test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.1 &mdash; Visualize a grid of hyperparameters\n",
    "\n",
    "In this exercise, you need to train an RBF SVM on the training data using different hyperparameter settings. The specific hyperparameters that you will inspect are *C* (the slack penalty) and the *gamma* (the RBF kernel spread).\n",
    "\n",
    "To specify a \"grid\" of values, it is enough to specify the specific \"ticks\" we'll enumerate along each dimension. For example, if we specified that *C* took values from $\\{C_1, C_2\\}$ and *gamma* took values from $\\{\\gamma_1, \\gamma_2, \\gamma_3\\}$ then we have specified the grid of hyperparameter values to try:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "(C_1,\\gamma_1) & (C_1,\\gamma_2) & (C_1,\\gamma_3)\\\\\n",
    "(C_2,\\gamma_1) & (C_2,\\gamma_2) & (C_2,\\gamma_3)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Follow the steps for the code cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Create a 4x4 grid made from 4 distinct *C* values and 4 distinct *gamma* values.**\n",
    "\n",
    "The values should be \"logarithmically spaced\", i.e., equally spaced on a logarithmic scale. Use **[numpy.logspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html)** to do this. For example, if you choose *C* values to be taken from $[1, 10, 100, 1000]$, these numbers are logarithmically spaced, not uniformly spaced. \n",
    "\n",
    "Why do we want logarithmic spacing? Because the *C* and *gamma* hyperparameters are sensiive over *orders of magnitude* and we don't know which order of magnitude is right for training on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2 lines, plus lines for printing the values, if it helps you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2. Print the training accuracy of an RBF SVM for each setting in the grid**\n",
    "\n",
    "Use for-loops to train an RBF SVM for each combination of (*C*, *gamma*) in your grid, using the variables you already created in the previous code cell. Train the SVMs on the *X_train* and *y_train* data from exercise 1.2.\n",
    "\n",
    "The output of your code cell should be something like:\n",
    "```\n",
    "72.9% training accuracy for C=1.0 gamma=0.01\n",
    "88.6% training accuracy for C=1.0 gamma=0.10\n",
    "90.0% training accuracy for C=1.0 gamma=1.00\n",
    "94.3% training accuracy for C=1.0 gamma=10.00\n",
    "88.6% training accuracy for C=10.0 gamma=0.01\n",
    "...\n",
    "```\n",
    "Based on your printed training accuracies, would you be able to guess which of these settings of (*C*, *gamma*) might work best on new test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 6-10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3. Plot the decision function for each setting in the grid**\n",
    "\n",
    "Repeat step 2 but instead of printing accuracies you should instead plot the data (use **plot_data**) and the decision function of the trained SVM model (use **plot_decision_function**). You must create a single figure with 16 subplots arranged in a 4x4 grid, one for each (*C*, *gamma*) hyperparameter setting.\n",
    "\n",
    "The top-left quadrant (the first 2x2 subplots) should look like the plot below.\n",
    "\n",
    "![image](img/SVM-CV.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 9-13 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.2 &mdash; Grid hyperparameter search \n",
    "\n",
    "In this exercise, you should use the [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function to perform a grid hyperparameter search (use the same grid of hyperparameters from *Exercise 2.1*) with the use of 3-fold cross validation. \n",
    "\n",
    "In particular, you should do the following:\n",
    "1. Create an **SVC** object to serve as a prototype of the kind of model you wish to train. Make it an RBF SVM.\n",
    "2. Define the **param_grid** argument to give **GridSearchCV**. This is a dictionary of the form *{ hyperparameter_name: array_of_possible_values }*. For example you could make a variable `param_grid = { 'C': ..., 'gamma': ...}` where `...` are the arrays of values you created early on in Exercise 2.1.\n",
    "3. Create a **GridSearchCV** object, passing your prototype SVM object and your param_grid as arguments.\n",
    "  * Set **verbose=1**, this will tell the function to print out more information of the grid search, and helps you to understand.  \n",
    "  * Use the **cv** parameter to specify 3-fold cross validation\n",
    "4. Call **fit** on the **GridSearchCV** object to perform the grid search. This will perform 3-fold cross validation for every combination of (*C*, *gamma*) in the grid you specified.\n",
    "5. Plot the data and the best SVM model from the grid search (use the **GridSearchCV** object's **best_estimator_** attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 6-10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) You may also want to:\n",
    "* print the validation score (accuracy, by default) of the best-performing hyperparameters by printing the **best_score_** attribute, and\n",
    "* inspect the best parameters using the **best_params_** attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "### Exercise 2.3 &mdash; Random hyperparameter search\n",
    "\n",
    "Grid hyperparameter search can be seen as an exhuastive search approach to find the best model congifuration. However, it is not necessar the best, especially in terms of computation efficiency. Thus, random hyperparameter search has been applied widely in various research studies.\n",
    "\n",
    "**<u>Random hyerperarameter search</u>** will accept a grid of values too, and will just randomly sample from the grid. But, random hyperparameter search **also** accepts statistical distributions from which it will sample. In this scenario, you can think of each distribution as defining an infinitely-dense grid dimension from which to sample. \n",
    "\n",
    "Choosing a distribution is more than choosing a min/max range. Just like listing grid values with linear spacing [1, 250, 500, 1000] or log spacing [1, 10, 100, 1000], different distributions offer different spacing. Other distributions, like Gaussian, don't even have a finite range when sampled.\n",
    "\n",
    "For example, if we sampled from a uniform distribution ([scipy.stats.uniform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html)) in range $[1, 1000]$, there's only a 1\\% chance that we'd sample a value between $1$ and $10$. Instead, we can sample from the *reciprocal* distribution ([scipy.stats.reciprocal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html)) which has logarithmic spacing between samples. A reciprocal distribution over range $[1, 1000]$ has the same chance of drawing a sample in range $[1, 10]$ as it does in range $[10, 100]$ or in range $[100, 1000]$. It is therefore also called a *log uniform* distribution.\n",
    "\n",
    "**Run the code cell below** to see the differences between sampling values from uniform and reciprocal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two objects, each representing a different random distribution\n",
    "reciprocal_distribution = scipy.stats.reciprocal(1, 100)  # Reciprocal distribution in range [0,100]\n",
    "uniform_distribution = scipy.stats.uniform(1, 100)        # Uniform distribution in range [0,100]\n",
    "\n",
    "# Draw 100,000 samples from each of the distributions\n",
    "np.random.seed(0)\n",
    "reciprocal_samples = reciprocal_distribution.rvs(100000);\n",
    "uniform_samples = uniform_distribution.rvs(100000);\n",
    "\n",
    "# Plot the density of samples from each distribution.\n",
    "plt.hist(uniform_samples, bins=50, label='uniform')\n",
    "plt.hist(reciprocal_samples, bins=50, label='reciprocal', alpha=0.8);\n",
    "plt.xticks([1, 10, 100])\n",
    "plt.yticks([])\n",
    "plt.xlabel('sampled value')\n",
    "plt.ylabel('density')\n",
    "plt.title(\"sample density of uniform vs reciprocal distributions\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, use **[sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)** to do a random hyperparameter search.\n",
    "\n",
    "In particular, you should do the following steps:\n",
    "1. Define the *param_distribution* argument of *RandomizedSearchCV*. This is similar to the *param_grid* you defined in Exercise 2.2 but, instead of specifying grid values, specify a reciprocal distribution to sample each hyperparameter from. \n",
    "2. Use *RandomizedSearchCV* to perform random hyperparameter search with 3-fold cross validation. The *RandomizedSearchCV* object will then draw a sample from each of those distributions when evaluating the next hyperparameters.\n",
    "  * Use argument *random_state*=0, *verbose*=1\n",
    "  * Set *n_iter*=16 in order to match the number of hyperparameters you evaluated with your 4x4 grid search.\n",
    "\n",
    "3. Print the best hyperparameters you found.\n",
    "4. Plot the data (use *plot_data*) and the best SVM model (use *plot_decision_function*).\n",
    "\n",
    "Your plot should look something like this:\n",
    "![image](img/random-search-niter-50.png)\n",
    "\n",
    "If your decision boundary looks more complex than the above plot, try increasing the *n_iter* parameter, because 16 random hyperparameter settings may not be enough to find a setting that has a good cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 8-10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supplementary exercise:** After you have random search working with a *reciprocal* distribution, try changing the distribution to be *uniform* over the same range. Does this help or harm the ability of random search to find a good hyperparameter setting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "### Exercise 2.4 &mdash; Evaluate hyperparameter performance on held-out test data\n",
    "\n",
    "After hyperparameter search is completed and the final hyperparameters are chosen, you can now do the final evaluation on the model performance using a **held-out test set** if one is available. You explicitly held out a test set ($\\mathbf{X}_\\text{test}, \\mathbf{y}_\\text{test}$) in Exercise 1.2, so use that data here.\n",
    "\n",
    "Using your *GridSearchCV* and *RandomSearchCV* objects from Exercises 2.2 and 2.3 respectively, print the training accuracy and test accuracy of the \"best estimator\" found by each.\n",
    "* Use the **best_estimator_** attribute to retrive a model that was trained on *all* the training data using the *best* hyperparameters (the hyperparameters with best average validation performance).\n",
    "* Use **sklearn.metrics.accuracy_score** or the **score** method on the search object (which uses the *best_estimator_*) to compute the accuracy on the training data and on the test data.\n",
    "\n",
    "Your output should look like:\n",
    "```\n",
    "grid search:\n",
    "  XX.X% train accuracy\n",
    "  XX.X% test accuracy\n",
    "random search:\n",
    "  XX.X% train accuracy\n",
    "  XX.X% test accuracy\n",
    "```\n",
    "*Tip:* Remember that if you want to print a `%` symbol when formatting a string, you must put `%%` in the original string so that Python knows it's not the start of a format specification (like `%d` or `%.2f`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 8-10 lines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
